# -*- coding: utf-8 -*-
"""Gradient_Boosting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_sV1hrcaQjCllVLS-NN2KQiOPTg6Ez1f

###Gradient Boosting
"""

# Define the parameter grid for Gradient Boosting
gb_param_grid = {
    'n_estimators': [100, 200, 300],  # Number of boosting stages
    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate shrinks the contribution of each tree
    'max_depth': [3, 4, 5]  # Maximum depth of the individual regression estimators
}

# Initialize Gradient Boosting model
gb_model = GradientBoostingRegressor(random_state=74)

# Initialize GridSearchCV for Gradient Boosting
grid_search_gb = GridSearchCV(estimator=gb_model, param_grid=gb_param_grid, cv=5, scoring='r2')

# Fit GridSearchCV to find the best parameters for Gradient Boosting
grid_search_gb.fit(X_train_scaled, y_train)

# Get the best parameters for Gradient Boosting
best_params_gb = grid_search_gb.best_params_
print("Best Parameters for Gradient Boosting:", best_params_gb)

# Use the best parameters to initialize the Gradient Boosting model
best_gb_model = GradientBoostingRegressor(**best_params_gb, random_state=74)

# Fit the model using the training data
best_gb_model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_gb = best_gb_model.predict(X_test_scaled)

# Evaluate the model
mse_gb = mean_squared_error(y_test, y_pred_gb)
rmse_gb = mean_squared_error(y_test, y_pred_gb, squared=False)  # Calculate RMSE
mae_gb = mean_absolute_error(y_test, y_pred_gb)
r2_gb = r2_score(y_test, y_pred_gb)

print("Gradient Boosting Model Metrics with Best Parameters:")
print("Mean Squared Error:", mse_gb)
print("Root Mean Squared Error:", rmse_gb)
print("Mean Absolute Error:", mae_gb)
print("R-squared Score:", r2_gb)
